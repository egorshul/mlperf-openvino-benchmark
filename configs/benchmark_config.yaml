# MLPerf v5.1 OpenVINO Benchmark Configuration
# This file contains all model and benchmark configurations

# Global settings
global:
  mlperf_version: "5.1"
  division: "closed"  # open or closed
  category: "datacenter"  # datacenter or edge

# Supported scenarios
scenarios:
  - Offline
  - Server

# Model configurations
models:
  resnet50:
    name: "ResNet50-v1.5"
    task: "image_classification"
    dataset: "imagenet2012"
    input_shape: [1, 3, 224, 224]
    input_name: "input"
    output_name: "output"
    data_format: "NCHW"
    dtype: "FP32"

    # MLPerf requirements
    accuracy_target: 0.7646  # 76.46% Top-1 accuracy
    accuracy_threshold: 0.99  # Must achieve 99% of reference accuracy

    # Preprocessing
    preprocessing:
      resize: [256, 256]
      center_crop: [224, 224]
      mean: [123.68, 116.78, 103.94]  # RGB order
      std: [1.0, 1.0, 1.0]
      channel_order: "RGB"

    # Model sources
    sources:
      onnx_url: "https://zenodo.org/record/4735647/files/resnet50_v1.onnx"
      onnx_md5: "b51de8b3c86c48bb7c1f6f8a7f8c72d7"

    # MLPerf LoadGen settings (from mlperf.conf v5.1)
    performance_sample_count: 1024  # QSL size for performance runs

    # Scenario-specific settings
    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 24576  # MLPerf official requirement
      samples_per_query: 50000  # All samples in one query

    server:
      target_latency_ns: 15000000  # 15ms
      target_qps: 10000.0  # High default for max throughput
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      # Note: Server mode uses min_duration, not min_query_count
      qsl_rng_seed: 13281865557512327830
      sample_index_rng_seed: 198141574272810017
      schedule_rng_seed: 7575108116881280410

  bert:
    name: "BERT-Large"
    task: "question_answering"
    dataset: "squad-1.1"
    input_shape:
      input_ids: [1, 384]
      attention_mask: [1, 384]
      token_type_ids: [1, 384]
    max_seq_length: 384
    dtype: "FP32"

    accuracy_target: 0.90874  # F1 score (official MLPerf v5.1)
    accuracy_threshold: 0.99

    sources:
      onnx_url: "https://zenodo.org/record/3733910/files/model.onnx"

    # MLPerf LoadGen settings (from mlperf.conf v5.1)
    performance_sample_count: 10833  # QSL size for performance runs

    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 10833  # MLPerf official requirement (SQuAD dataset size)
      samples_per_query: 10833

    server:
      target_latency_ns: 130000000  # 130ms
      target_qps: 5000.0  # High default for max throughput
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)

  retinanet:
    name: "RetinaNet"
    task: "object_detection"
    dataset: "openimages-800"
    input_shape: [1, 3, 800, 800]
    input_name: "image"
    data_format: "NCHW"
    dtype: "FP32"

    # Output tensor names (MLPerf RetinaNet ONNX model)
    output_names:
      boxes: "boxes"      # [N, 4] - x1, y1, x2, y2
      scores: "scores"    # [N] - confidence scores
      labels: "labels"    # [N] - class labels

    # MLPerf closed division accuracy target
    accuracy_target: 0.3755  # mAP (37.55% = 99% of 37.93% reference)
    accuracy_threshold: 0.99

    # Preprocessing (closed division requirements)
    preprocessing:
      resize: [800, 800]
      mean: [103.53, 116.28, 123.675]  # BGR order (OpenCV default)
      std: [57.375, 57.12, 58.395]     # BGR order
      channel_order: "BGR"
      output_layout: "NHWC"  # Convert to NHWC for NPU efficiency

    # Model sources (official MLPerf model)
    sources:
      onnx_url: "https://zenodo.org/record/6617879/files/resnext50_32x4d_fpn.onnx"

    # MLPerf LoadGen settings (from mlperf.conf)
    performance_sample_count: 64  # QSL size for performance runs

    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 24576  # MLPerf official requirement

    server:
      target_latency_ns: 100000000  # 100ms (from mlperf.conf)
      target_qps: 1000.0  # Starting point, auto-tuned
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)

  whisper:
    name: "Whisper-Large-v3"
    task: "speech_recognition"
    dataset: "librispeech"
    input_shape: [1, 80, 3000]  # (batch, n_mels, time_frames)
    input_name: "input_features"
    output_name: "sequences"
    data_format: "NCT"  # batch, channels (mels), time
    dtype: "FP32"

    # Audio preprocessing parameters
    audio:
      sample_rate: 16000
      n_fft: 400
      hop_length: 160
      n_mels: 80
      chunk_length_sec: 30

    # Generation parameters
    generation:
      max_new_tokens: 448
      language: "en"
      task: "transcribe"
      no_timestamps: true

    accuracy_target: 0.979329  # Word Accuracy (official MLPerf v5.1)
    accuracy_threshold: 0.99  # Must achieve 99% of reference accuracy (BF16)

    sources:
      huggingface_model_id: "openai/whisper-large-v3"

    # MLPerf LoadGen settings (from mlperf.conf v5.1)
    performance_sample_count: 1633  # QSL size for performance runs

    # Whisper supports Offline scenario only (per MLCommons specification)
    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 1633  # MLPerf v5.1 official requirement
      samples_per_query: 1633

  sdxl:
    name: "Stable-Diffusion-XL"
    task: "text_to_image"
    dataset: "coco2014"
    input_shape: [1, 77]  # (batch, max_token_length) for text input
    input_name: "input_ids"
    output_name: "sample"
    data_format: "NC"
    dtype: "FP32"

    # Image generation parameters (MLCommons reference)
    generation:
      image_size: 1024
      guidance_scale: 8.0
      num_inference_steps: 20
      negative_prompt: "normal quality, low quality, worst quality, low res, blurry, nsfw, nude"

    # MLPerf v5.1 accuracy targets for SDXL (closed division)
    # CLIP_SCORE: >= 31.68632 and <= 31.81332
    # FID_SCORE: >= 23.01086 and <= 23.95007
    accuracy_target: 31.68632  # Minimum CLIP score
    accuracy_threshold: 1.0  # Must be within range

    accuracy_metrics:
      clip_score_min: 31.68632
      clip_score_max: 31.81332
      fid_score_min: 23.01086
      fid_score_max: 23.95007

    sources:
      huggingface_model_id: "stabilityai/stable-diffusion-xl-base-1.0"

    # MLPerf LoadGen settings (from mlperf.conf v5.1)
    performance_sample_count: 5000  # QSL size for performance runs

    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 5000  # MLPerf official requirement (COCO subset)
      samples_per_query: 1

    server:
      target_latency_ns: 20000000000  # 20 seconds for image generation
      target_qps: 10.0  # Lower QPS due to high latency
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)

# OpenVINO specific settings
openvino:
  device: "CPU"
  num_streams: "AUTO"  # AUTO, or specific number
  num_threads: 0  # 0 = auto-detect
  batch_size: 1  # Inference batch size (increase for higher throughput)
  enable_profiling: false
  cache_dir: "./cache"
  
  # Performance hints
  performance_hint: "THROUGHPUT"  # THROUGHPUT or LATENCY
  
  # Precision options
  inference_precision: "FP32"  # FP32, FP16, INT8
  
  # CPU-specific options
  cpu:
    bind_thread: true
    threads_per_stream: 0  # 0 = auto
    enable_hyper_threading: true

# LoadGen settings
loadgen:
  # Test modes
  test_mode: "PerformanceOnly"  # AccuracyOnly, PerformanceOnly, FindPeakPerformance

  # Logging
  log_output_path: "./results"
  log_copy_summary: true
  log_enable_trace: false

  # Performance settings
  # Note: performance_sample_count is model-specific, see each model's config
  target_qps: 0  # 0 = auto-discover

  # Accuracy settings
  accuracy_log_rng_seed: 0
  accuracy_log_probability: 0.0
  accuracy_log_sampling_target: 0

# Dataset paths
datasets:
  imagenet2012:
    path: "./data/imagenet"
    val_map: "./data/imagenet/val_map.txt"
    calibration_path: "./data/imagenet/calibration"
  
  squad:
    path: "./data/squad"
    dev_file: "dev-v1.1.json"
    vocab_file: "vocab.txt"
  
  openimages:
    path: "./data/openimages"
    annotations: "annotations/validation-annotations-bbox.csv"
  
  librispeech:
    path: "./data/librispeech"
    subset: "dev-clean"  # dev-clean, dev-other, test-clean, test-other
    transcript_file: "transcripts.txt"

  coco2014:
    path: "./data/coco2014"
    prompts_file: "coco-1024.tsv"
    captions_file: "annotations/captions_val2014.json"
    images_dir: "coco-1024"  # Optional reference images for FID

# Output settings
output:
  results_dir: "./results"
  logs_dir: "./logs"
  reports_dir: "./reports"
  
  # Report formats
  generate_html_report: true
  generate_json_report: true
  generate_csv_report: true
