# MLPerf v5.1 OpenVINO Benchmark Configuration
# This file contains all model and benchmark configurations

# Global settings
global:
  mlperf_version: "5.1"
  division: "closed"  # open or closed
  category: "datacenter"  # datacenter or edge

# Supported scenarios
scenarios:
  - Offline
  - Server

# Model configurations
models:
  resnet50:
    name: "ResNet50-v1.5"
    task: "image_classification"
    dataset: "imagenet2012"
    input_shape: [1, 3, 224, 224]
    input_name: "input"
    output_name: "output"
    data_format: "NCHW"
    dtype: "FP32"

    # MLPerf requirements
    accuracy_target: 0.7646  # 76.46% Top-1 accuracy
    accuracy_threshold: 0.99  # Must achieve 99% of reference accuracy

    # Preprocessing
    preprocessing:
      resize: [256, 256]
      center_crop: [224, 224]
      mean: [123.68, 116.78, 103.94]  # RGB order
      std: [1.0, 1.0, 1.0]
      channel_order: "RGB"

    # Model sources
    sources:
      onnx_url: "https://zenodo.org/record/4735647/files/resnet50_v1.onnx"
      onnx_md5: "b51de8b3c86c48bb7c1f6f8a7f8c72d7"

    # MLPerf LoadGen settings (from mlperf.conf v5.1)
    performance_sample_count: 1024  # QSL size for performance runs

    # Scenario-specific settings
    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 24576  # MLPerf official requirement
      samples_per_query: 50000  # All samples in one query

    server:
      target_latency_ns: 15000000  # 15ms
      target_qps: 10000.0  # High default for max throughput
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      # Note: Server mode uses min_duration, not min_query_count
      qsl_rng_seed: 13281865557512327830
      sample_index_rng_seed: 198141574272810017
      schedule_rng_seed: 7575108116881280410

  bert:
    name: "BERT-Large"
    task: "question_answering"
    dataset: "squad-1.1"
    input_shape:
      input_ids: [1, 384]
      attention_mask: [1, 384]
      token_type_ids: [1, 384]
    max_seq_length: 384
    dtype: "FP32"

    accuracy_target: 0.90874  # F1 score (official MLPerf v5.1)
    accuracy_threshold: 0.99

    sources:
      onnx_url: "https://zenodo.org/record/3733910/files/model.onnx"

    # MLPerf LoadGen settings (from mlperf.conf v5.1)
    performance_sample_count: 10833  # QSL size for performance runs

    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 10833  # MLPerf official requirement (SQuAD dataset size)
      samples_per_query: 10833

    server:
      target_latency_ns: 130000000  # 130ms
      target_qps: 5000.0  # High default for max throughput
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)

  retinanet:
    name: "RetinaNet"
    task: "object_detection"
    dataset: "openimages-800"
    input_shape: [1, 3, 800, 800]
    input_name: "image"
    data_format: "NCHW"
    dtype: "FP32"

    # Output tensor names (MLPerf RetinaNet ONNX model)
    output_names:
      boxes: "boxes"      # [N, 4] - x1, y1, x2, y2
      scores: "scores"    # [N] - confidence scores
      labels: "labels"    # [N] - class labels

    # MLPerf closed division accuracy target
    accuracy_target: 0.3755  # mAP (37.55% = 99% of 37.93% reference)
    accuracy_threshold: 0.99

    # Preprocessing (closed division requirements)
    preprocessing:
      resize: [800, 800]
      mean: [103.53, 116.28, 123.675]  # BGR order (OpenCV default)
      std: [57.375, 57.12, 58.395]     # BGR order
      channel_order: "BGR"
      output_layout: "NHWC"  # Convert to NHWC for NPU efficiency

    # Model sources (official MLPerf model)
    sources:
      onnx_url: "https://zenodo.org/record/6617879/files/resnext50_32x4d_fpn.onnx"

    # MLPerf LoadGen settings (from mlperf.conf)
    performance_sample_count: 64  # QSL size for performance runs

    offline:
      min_duration_ms: 60000  # MLPerf v5.1 official: 600000 (10 min)
      min_query_count: 500  # MLPerf v5.1 official: 24576
      target_qps: 10.0  # MLPerf v5.1 official: not set (defaults to 1000)

    server:
      target_latency_ns: 100000000  # 100ms (from mlperf.conf)
      target_qps: 10.0  # MLPerf v5.1 official: 1000.0
      min_duration_ms: 60000  # MLPerf v5.1 official: 600000 (10 min)
      min_query_count: 500  # MLPerf v5.1 official: 24576

  whisper:
    name: "Whisper-Large-v3"
    task: "speech_recognition"
    dataset: "librispeech"
    input_shape: [1, 80, 3000]  # (batch, n_mels, time_frames)
    input_name: "input_features"
    output_name: "sequences"
    data_format: "NCT"  # batch, channels (mels), time
    dtype: "FP32"

    # Audio preprocessing parameters
    audio:
      sample_rate: 16000
      n_fft: 400
      hop_length: 160
      n_mels: 80
      chunk_length_sec: 30

    # Generation parameters
    generation:
      max_new_tokens: 448
      language: "en"
      task: "transcribe"
      no_timestamps: true

    accuracy_target: 0.979329  # Word Accuracy (official MLPerf v5.1)
    accuracy_threshold: 0.99  # Must achieve 99% of reference accuracy (BF16)

    sources:
      huggingface_model_id: "openai/whisper-large-v3"

    # MLPerf LoadGen settings (from mlperf.conf v5.1)
    performance_sample_count: 1633  # QSL size for performance runs

    # Whisper supports Offline scenario only (per MLCommons specification)
    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 1633  # MLPerf v5.1 official requirement
      samples_per_query: 1633

  ssd-resnet34:
    name: "SSD-ResNet34"
    task: "object_detection"
    dataset: "coco2017"
    input_shape: [1, 3, 1200, 1200]
    input_name: "image"
    data_format: "NCHW"
    dtype: "FP32"

    # Output tensor names (MLPerf SSD-ResNet34 ONNX model)
    output_names:
      bboxes: "bboxes"    # [N, 4] - ltrb format
      labels: "labels"    # [N] - class labels (0-80)
      scores: "scores"    # [N] - confidence scores

    # MLPerf closed division accuracy target
    accuracy_target: 0.20  # mAP 20.0% (99% of reference = 19.8%)
    accuracy_threshold: 0.99

    # Preprocessing (closed division requirements)
    preprocessing:
      resize: [1200, 1200]
      center_crop: [1200, 1200]
      # SSD-ResNet34 uses ImageNet mean/std (normalized to [0,1])
      mean: [0.485, 0.456, 0.406]  # RGB order
      std: [0.229, 0.224, 0.225]   # RGB order
      channel_order: "RGB"
      output_layout: "NHWC"  # Convert to NHWC for NPU efficiency

    # Model sources (official MLPerf model)
    sources:
      onnx_url: "https://zenodo.org/record/4735664/files/ssd_resnet34_mAP_20.2.onnx"

    # MLPerf LoadGen settings
    performance_sample_count: 256  # QSL size for performance runs

    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf official requirement)
      min_query_count: 24576   # MLPerf official requirement
      samples_per_query: 1

    server:
      target_latency_ns: 100000000  # 100ms (MLPerf official)
      target_qps: 100.0
      min_duration_ms: 600000  # 10 minutes (MLPerf official requirement)
      min_query_count: 24576

  sdxl:
    name: "Stable-Diffusion-XL"
    task: "text_to_image"
    dataset: "coco2014"
    input_shape: [1, 77]  # (batch, max_token_length) for text input
    input_name: "input_ids"
    output_name: "sample"
    data_format: "NC"
    dtype: "FP32"

    # Image generation parameters (MLCommons reference)
    generation:
      image_size: 1024
      guidance_scale: 8.0
      num_inference_steps: 20
      negative_prompt: "normal quality, low quality, worst quality, low res, blurry, nsfw, nude"

    # MLPerf v5.1 accuracy targets for SDXL (closed division)
    # CLIP_SCORE: >= 31.68632 and <= 31.81332
    # FID_SCORE: >= 23.01086 and <= 23.95007
    accuracy_target: 31.68632  # Minimum CLIP score
    accuracy_threshold: 1.0  # Must be within range

    accuracy_metrics:
      clip_score_min: 31.68632
      clip_score_max: 31.81332
      fid_score_min: 23.01086
      fid_score_max: 23.95007

    sources:
      huggingface_model_id: "stabilityai/stable-diffusion-xl-base-1.0"

    # MLPerf LoadGen settings (from mlperf.conf v5.1)
    performance_sample_count: 5000  # QSL size for performance runs

    offline:
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)
      min_query_count: 5000  # MLPerf official requirement (COCO subset)
      samples_per_query: 1

    server:
      target_latency_ns: 20000000000  # 20 seconds for image generation
      target_qps: 10.0  # Lower QPS due to high latency
      min_duration_ms: 600000  # 10 minutes (MLPerf v5.1 official requirement)

  llama3.1-8b:
    name: "Llama-3.1-8B"
    task: "text_generation"
    dataset: "cnn-dailymail"
    input_shape: [1, 8000]  # (batch, model_max_length per MLCommons reference)
    input_name: "input_ids"
    output_name: "logits"
    data_format: "NC"
    dtype: "FP16"

    # Generation parameters (MLCommons Inference v5.1 reference)
    # Task: text summarization (CNN-DailyMail)
    generation:
      max_new_tokens: 128    # Summarization output is short
      min_new_tokens: 1
      do_sample: false       # Greedy decoding per MLPerf spec
      num_beams: 1

    # MLPerf v5.1 accuracy targets (ROUGE fmeasure * 100)
    # Reference values from FP32 baseline; 99% threshold applied
    accuracy_target: 0.0     # Composite - individual ROUGE thresholds below
    accuracy_threshold: 0.99

    accuracy_metrics:
      rouge1: 38.7792        # FP32 reference (99% = 38.3914)
      rouge2: 15.9075        # FP32 reference (99% = 15.7484)
      rougeL: 24.4957        # FP32 reference (99% = 24.2507)
      rougeLsum: 35.793      # FP32 reference (99% = 35.4351)
      gen_len: 8167644       # FP32 reference gen_len (sum of char lengths, 90% threshold)

    sources:
      huggingface_model_id: "meta-llama/Meta-Llama-3.1-8B-Instruct"

    # MLPerf LoadGen settings (from mlperf.conf)
    performance_sample_count: 13368    # = total CNN-DailyMail eval samples
    use_token_latencies: true          # Required for LLM benchmarks

    offline:
      min_duration_ms: 600000          # 10 minutes (MLPerf v5.1)
      min_query_count: 13368
      samples_per_query: 1

    server:
      target_latency_ns: 20000000000   # 20s TTFT per MLPerf constants.py
      target_qps: 1.0
      min_duration_ms: 600000          # 10 minutes (MLPerf v5.1)
      min_query_count: 270336          # MLPerf constants.py min-queries Server

  llama2-70b:
    name: "Llama-2-70B"
    task: "text_generation"
    dataset: "open-orca"
    input_shape: [1, 1024]  # (batch, max_seq_length per MLCommons reference)
    input_name: "input_ids"
    output_name: "logits"
    data_format: "NC"
    dtype: "FP16"

    # Generation parameters (MLCommons Inference reference)
    # Task: text generation (OpenOrca)
    generation:
      max_new_tokens: 1024     # Much longer generation than Llama 3.1 8B (128)
      min_new_tokens: 1
      do_sample: false         # Greedy decoding per MLPerf spec
      num_beams: 1
      early_stopping: true

    # MLPerf accuracy targets (ROUGE fmeasure * 100)
    # Reference values from FP16 DGX-H100 baseline; 99% threshold applied
    accuracy_target: 0.0      # Composite - individual ROUGE thresholds below
    accuracy_threshold: 0.99

    accuracy_metrics:
      rouge1: 44.4312          # FP16 reference (99% = 43.9869)
      rouge2: 22.0352          # FP16 reference (99% = 21.8148)
      rougeL: 28.6162          # FP16 reference (99% = 28.3300)
      tokens_per_sample: 294.45  # 90%-110% window (265.005 - 323.895)

    sources:
      huggingface_model_id: "meta-llama/Llama-2-70b-chat-hf"

    # MLPerf LoadGen settings
    performance_sample_count: 24576    # = total OpenOrca eval samples
    use_token_latencies: true          # Required for LLM benchmarks

    offline:
      min_duration_ms: 600000          # 10 minutes (MLPerf official)
      min_query_count: 24576
      samples_per_query: 1

    server:
      target_latency_ns: 20000000000   # 20s TTFT
      target_qps: 1.0
      min_duration_ms: 600000          # 10 minutes (MLPerf official)
      min_query_count: 270336          # MLPerf constants.py

# OpenVINO specific settings
openvino:
  device: "CPU"
  num_streams: "AUTO"  # AUTO, or specific number
  num_threads: 0  # 0 = auto-detect
  batch_size: 1  # Inference batch size (increase for higher throughput)
  enable_profiling: false
  cache_dir: "./cache"
  
  # Performance hints
  performance_hint: "THROUGHPUT"  # THROUGHPUT or LATENCY
  
  # Precision options
  inference_precision: "FP32"  # FP32, FP16, INT8
  
  # CPU-specific options
  cpu:
    bind_thread: true
    threads_per_stream: 0  # 0 = auto
    enable_hyper_threading: true

# LoadGen settings
loadgen:
  # Test modes
  test_mode: "PerformanceOnly"  # AccuracyOnly, PerformanceOnly, FindPeakPerformance

  # Logging
  log_output_path: "./results"
  log_copy_summary: true
  log_enable_trace: false

  # Performance settings
  # Note: performance_sample_count is model-specific, see each model's config
  target_qps: 0  # 0 = auto-discover

  # Accuracy settings
  accuracy_log_rng_seed: 0
  accuracy_log_probability: 0.0
  accuracy_log_sampling_target: 0

# Dataset paths
datasets:
  imagenet2012:
    path: "./data/imagenet"
    val_map: "./data/imagenet/val_map.txt"
    calibration_path: "./data/imagenet/calibration"
  
  squad:
    path: "./data/squad"
    dev_file: "dev-v1.1.json"
    vocab_file: "vocab.txt"
  
  openimages:
    path: "./data/openimages"
    annotations: "annotations/validation-annotations-bbox.csv"
  
  librispeech:
    path: "./data/librispeech"
    subset: "dev-clean"  # dev-clean, dev-other, test-clean, test-other
    transcript_file: "transcripts.txt"

  coco2017:
    path: "./data/coco2017"
    annotations: "annotations/instances_val2017.json"
    images_dir: "val2017"

  coco2014:
    path: "./data/coco2014"
    prompts_file: "coco-1024.tsv"
    captions_file: "annotations/captions_val2014.json"
    images_dir: "coco-1024"  # Optional reference images for FID

  cnn-dailymail:
    path: "./data/cnn-dailymail"
    eval_file: "cnn_eval.json"          # 13,368 samples (datacenter)
    edge_file: "sample_cnn_eval_5000.json"  # 5,000 samples (edge)

  open-orca:
    path: "./data/open-orca"
    eval_file: "open_orca_gpt4_tokenized_llama.sampled_24576.pkl"  # 24,576 samples
    calibration_file: "open_orca_gpt4_tokenized_llama.calibration_1000.pkl"  # 1,000 samples

# Output settings
output:
  results_dir: "./results"
  logs_dir: "./logs"
  reports_dir: "./reports"
  
  # Report formats
  generate_html_report: true
  generate_json_report: true
  generate_csv_report: true
